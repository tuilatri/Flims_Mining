{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e824d869",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline\n",
    "## Anime Recommendation System - Kaggle Dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Setup and Import Libraries\n",
    "\n",
    "**Objective:** Import all necessary Python libraries for data manipulation, analysis, and visualization.\n",
    "\n",
    "**Libraries Used:**\n",
    "- pandas - Data manipulation and CSV handling\n",
    "- numpy - Numerical operations\n",
    "- matplotlib & seaborn - Data visualization\n",
    "- pathlib - File path management\n",
    "- warnings - Suppress warning messages\n",
    "\n",
    "**Expected Output:** No errors, all libraries loaded successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe3e604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n",
      "✓ Pandas version: 2.3.3\n",
      "✓ NumPy version: 2.3.5\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd              # Data manipulation\n",
    "import numpy as np               # Numerical operations\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import seaborn as sns           # Advanced visualizations\n",
    "from pathlib import Path        # File path handling\n",
    "import warnings                 # Warning control\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 100)      # Show up to 100 rows\n",
    "\n",
    "# Verify imports\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8499816",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load Raw Data\n",
    "\n",
    "**Objective:** Load the raw anime and rating datasets from CSV files.\n",
    "\n",
    "**Input Files:**\n",
    "- ../data/raw/anime.csv - Contains anime information (7 columns)\n",
    "- ../data/raw/rating.csv - Contains user ratings (3 columns)\n",
    "\n",
    "**Expected Output:** \n",
    "- Two dataframes loaded successfully\n",
    "- Display of dataset shapes (rows x columns)\n",
    "- No loading errors\n",
    "\n",
    "**Note:** Using encoding='utf-8' for anime.csv to handle special characters in anime names (Japanese characters, symbols, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8125a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING RAW DATA\n",
      "============================================================\n",
      "\n",
      "✓ Anime dataset loaded: 12,294 rows × 7 columns\n",
      "✓ Rating dataset loaded: 7,813,737 rows × 3 columns\n",
      "\n",
      "--- Anime Data Preview ---\n",
      "   anime_id                              name  \\\n",
      "0     32281                    Kimi no Na wa.   \n",
      "1      5114  Fullmetal Alchemist: Brotherhood   \n",
      "2     28977                          Gintama°   \n",
      "\n",
      "                                               genre   type episodes  rating  \\\n",
      "0               Drama, Romance, School, Supernatural  Movie        1    9.37   \n",
      "1  Action, Adventure, Drama, Fantasy, Magic, Mili...     TV       64    9.26   \n",
      "2  Action, Comedy, Historical, Parody, Samurai, S...     TV       51    9.25   \n",
      "\n",
      "   members  \n",
      "0   200630  \n",
      "1   793665  \n",
      "2   114262  \n",
      "\n",
      "--- Rating Data Preview ---\n",
      "   user_id  anime_id  rating\n",
      "0        1        20      -1\n",
      "1        1        24      -1\n",
      "2        1        79      -1\n"
     ]
    }
   ],
   "source": [
    "# Load raw datasets\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING RAW DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load anime dataset with UTF-8 encoding for special characters\n",
    "anime_raw = pd.read_csv('../data/raw/anime.csv', encoding='utf-8')\n",
    "\n",
    "# Load rating dataset\n",
    "rating_raw = pd.read_csv('../data/raw/rating.csv')\n",
    "\n",
    "# Display shapes\n",
    "print(f\"\\n✓ Anime dataset loaded: {anime_raw.shape[0]:,} rows × {anime_raw.shape[1]} columns\")\n",
    "print(f\"✓ Rating dataset loaded: {rating_raw.shape[0]:,} rows × {rating_raw.shape[1]} columns\")\n",
    "\n",
    "# Quick preview of data\n",
    "print(\"\\n--- Anime Data Preview ---\")\n",
    "print(anime_raw.head(3))\n",
    "\n",
    "print(\"\\n--- Rating Data Preview ---\")\n",
    "print(rating_raw.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d78902",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Initial Data Exploration\n",
    "\n",
    "**Objective:** Understand the structure, data types, and quality of raw data before cleaning.\n",
    "\n",
    "**Analysis Performed:**\n",
    "- Column names and data types\n",
    "- Missing values count per column\n",
    "- Basic statistical summary (min, max, mean, etc.)\n",
    "- Special case: Count of -1 ratings (watched but not rated)\n",
    "\n",
    "**Purpose:** Identify data quality issues that need to be addressed in cleaning phase.\n",
    "\n",
    "**Expected Findings:**\n",
    "- Missing values in anime data (genre, type, episodes, rating)\n",
    "- -1 ratings in rating data (need to be removed)\n",
    "- Data type validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5b1561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIAL DATA EXPLORATION\n",
      "============================================================\n",
      "\n",
      "--- ANIME DATASET ---\n",
      "\n",
      "Columns: ['anime_id', 'name', 'genre', 'type', 'episodes', 'rating', 'members']\n",
      "\n",
      "Data Types:\n",
      "anime_id      int64\n",
      "name         object\n",
      "genre        object\n",
      "type         object\n",
      "episodes     object\n",
      "rating      float64\n",
      "members       int64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "anime_id      0\n",
      "name          0\n",
      "genre        62\n",
      "type         25\n",
      "episodes      0\n",
      "rating      230\n",
      "members       0\n",
      "dtype: int64\n",
      "\n",
      "Basic Statistics:\n",
      "           anime_id        rating       members\n",
      "count  12294.000000  12064.000000  1.229400e+04\n",
      "mean   14058.221653      6.473902  1.807134e+04\n",
      "std    11455.294701      1.026746  5.482068e+04\n",
      "min        1.000000      1.670000  5.000000e+00\n",
      "25%     3484.250000      5.880000  2.250000e+02\n",
      "50%    10260.500000      6.570000  1.550000e+03\n",
      "75%    24794.500000      7.180000  9.437000e+03\n",
      "max    34527.000000     10.000000  1.013917e+06\n",
      "\n",
      "--- RATING DATASET ---\n",
      "\n",
      "Columns: ['user_id', 'anime_id', 'rating']\n",
      "\n",
      "Data Types:\n",
      "user_id     int64\n",
      "anime_id    int64\n",
      "rating      int64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "user_id     0\n",
      "anime_id    0\n",
      "rating      0\n",
      "dtype: int64\n",
      "\n",
      "Basic Statistics:\n",
      "            user_id      anime_id        rating\n",
      "count  7.813737e+06  7.813737e+06  7.813737e+06\n",
      "mean   3.672796e+04  8.909072e+03  6.144030e+00\n",
      "std    2.099795e+04  8.883950e+03  3.727800e+00\n",
      "min    1.000000e+00  1.000000e+00 -1.000000e+00\n",
      "25%    1.897400e+04  1.240000e+03  6.000000e+00\n",
      "50%    3.679100e+04  6.213000e+03  7.000000e+00\n",
      "75%    5.475700e+04  1.409300e+04  9.000000e+00\n",
      "max    7.351600e+04  3.451900e+04  1.000000e+01\n",
      "\n",
      "⚠ Ratings with -1 (watched but not rated): 1,476,496 (18.90%)\n",
      "\n",
      "--- SUMMARY ---\n",
      "Anime with missing ratings: 230\n",
      "Anime with missing genres: 62\n",
      "Total missing values in rating data: 0\n"
     ]
    }
   ],
   "source": [
    "# Explore raw data structure and quality\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INITIAL DATA EXPLORATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- ANIME DATASET EXPLORATION ---\n",
    "print(\"\\n--- ANIME DATASET ---\")\n",
    "print(f\"\\nColumns: {anime_raw.columns.tolist()}\")\n",
    "print(f\"\\nData Types:\\n{anime_raw.dtypes}\")\n",
    "print(f\"\\nMissing Values:\\n{anime_raw.isnull().sum()}\")\n",
    "print(f\"\\nBasic Statistics:\\n{anime_raw.describe()}\")\n",
    "\n",
    "# --- RATING DATASET EXPLORATION ---\n",
    "print(\"\\n--- RATING DATASET ---\")\n",
    "print(f\"\\nColumns: {rating_raw.columns.tolist()}\")\n",
    "print(f\"\\nData Types:\\n{rating_raw.dtypes}\")\n",
    "print(f\"\\nMissing Values:\\n{rating_raw.isnull().sum()}\")\n",
    "print(f\"\\nBasic Statistics:\\n{rating_raw.describe()}\")\n",
    "\n",
    "# Check for -1 ratings (special case: watched but not rated)\n",
    "unrated_count = (rating_raw['rating'] == -1).sum()\n",
    "unrated_percent = (unrated_count / len(rating_raw)) * 100\n",
    "print(f\"\\n⚠ Ratings with -1 (watched but not rated): {unrated_count:,} ({unrated_percent:.2f}%)\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n--- SUMMARY ---\")\n",
    "print(f\"Anime with missing ratings: {anime_raw['rating'].isnull().sum():,}\")\n",
    "print(f\"Anime with missing genres: {anime_raw['genre'].isnull().sum():,}\")\n",
    "print(f\"Total missing values in rating data: {rating_raw.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27b33d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Clean Anime Dataset (Complete)\n",
    "\n",
    "**Objective:** Perform all anime dataset cleaning operations in one comprehensive step.\n",
    "\n",
    "**Cleaning Operations:**\n",
    "1. Remove duplicate rows\n",
    "2. Remove rows with missing names\n",
    "3. Fill missing genres with 'Unknown'\n",
    "4. Fill missing types with 'Unknown'\n",
    "5. Handle missing/unknown episodes (convert to 0)\n",
    "6. Remove rows with missing ratings (critical field)\n",
    "7. Remove invalid ratings (outside 0-10 range)\n",
    "8. Handle missing members and convert to integer\n",
    "9. Remove anime with <10 members (too obscure)\n",
    "10. Clean whitespace from text columns\n",
    "11. Reset index\n",
    "\n",
    "**Rationale:** All cleaning operations combined for efficiency and clarity.\n",
    "\n",
    "**Expected Result:** Fully cleaned anime dataset ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d26d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLEANING ANIME DATASET (COMPLETE)\n",
      "============================================================\n",
      "\n",
      "Starting with: 12,294 rows\n",
      "\n",
      "1. Duplicates removed: 0\n",
      "2. No missing names found ✓\n",
      "3. Missing genres filled with 'Unknown': 62\n",
      "4. Missing types filled with 'Unknown': 25\n",
      "5. Missing/Unknown episodes filled with 0: 0\n",
      "6. Rows with missing ratings removed: 230\n",
      "7. Invalid ratings removed: 0\n",
      "8. Missing members filled with 0: 0\n",
      "9. Anime with <10 members removed: 0\n",
      "10. Whitespace cleaned from text columns ✓\n",
      "11. Index reset ✓\n",
      "\n",
      "--- ANIME DATASET CLEANING COMPLETE ---\n",
      "Original: 12,294 rows\n",
      "Cleaned: 12,064 rows\n",
      "Removed: 230 rows (1.87%)\n",
      "\n",
      "Missing values: 0\n",
      "Duplicates: 0\n",
      "\n",
      "--- Sample of Cleaned Data ---\n",
      "   anime_id                              name  \\\n",
      "0     32281                    Kimi no Na wa.   \n",
      "1      5114  Fullmetal Alchemist: Brotherhood   \n",
      "2     28977                          Gintama°   \n",
      "\n",
      "                                               genre   type  episodes  rating  \\\n",
      "0               Drama, Romance, School, Supernatural  Movie         1    9.37   \n",
      "1  Action, Adventure, Drama, Fantasy, Magic, Mili...     TV        64    9.26   \n",
      "2  Action, Comedy, Historical, Parody, Samurai, S...     TV        51    9.25   \n",
      "\n",
      "   members  \n",
      "0   200630  \n",
      "1   793665  \n",
      "2   114262  \n"
     ]
    }
   ],
   "source": [
    "# Complete anime dataset cleaning\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLEANING ANIME DATASET (COMPLETE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a copy to preserve original data\n",
    "anime_cleaned = anime_raw.copy()\n",
    "print(f\"\\nStarting with: {len(anime_cleaned):,} rows\")\n",
    "\n",
    "# 1. Remove duplicates\n",
    "duplicates_count = anime_cleaned.duplicated().sum()\n",
    "anime_cleaned = anime_cleaned.drop_duplicates()\n",
    "print(f\"\\n1. Duplicates removed: {duplicates_count:,}\")\n",
    "\n",
    "# 2. Remove rows with missing names\n",
    "missing_names = anime_cleaned['name'].isnull().sum()\n",
    "if missing_names > 0:\n",
    "    anime_cleaned = anime_cleaned.dropna(subset=['name'])\n",
    "    print(f\"2. Rows with missing names removed: {missing_names:,}\")\n",
    "else:\n",
    "    print(f\"2. No missing names found ✓\")\n",
    "\n",
    "# 3. Fill missing genres\n",
    "missing_genre = anime_cleaned['genre'].isnull().sum()\n",
    "anime_cleaned['genre'] = anime_cleaned['genre'].fillna('Unknown')\n",
    "print(f\"3. Missing genres filled with 'Unknown': {missing_genre:,}\")\n",
    "\n",
    "# 4. Fill missing types\n",
    "missing_type = anime_cleaned['type'].isnull().sum()\n",
    "anime_cleaned['type'] = anime_cleaned['type'].fillna('Unknown')\n",
    "print(f\"4. Missing types filled with 'Unknown': {missing_type:,}\")\n",
    "\n",
    "# 5. Handle missing/unknown episodes\n",
    "missing_episodes = anime_cleaned['episodes'].isnull().sum()\n",
    "anime_cleaned['episodes'] = pd.to_numeric(anime_cleaned['episodes'], errors='coerce')\n",
    "anime_cleaned['episodes'] = anime_cleaned['episodes'].fillna(0)\n",
    "anime_cleaned['episodes'] = anime_cleaned['episodes'].astype(int)\n",
    "print(f\"5. Missing/Unknown episodes filled with 0: {missing_episodes:,}\")\n",
    "\n",
    "# 6. Remove rows with missing ratings (CRITICAL)\n",
    "missing_ratings = anime_cleaned['rating'].isnull().sum()\n",
    "anime_cleaned = anime_cleaned.dropna(subset=['rating'])\n",
    "print(f\"6. Rows with missing ratings removed: {missing_ratings:,}\")\n",
    "\n",
    "# 7. Remove invalid ratings\n",
    "invalid_low = (anime_cleaned['rating'] < 0).sum()\n",
    "invalid_high = (anime_cleaned['rating'] > 10).sum()\n",
    "anime_cleaned = anime_cleaned[(anime_cleaned['rating'] >= 0) & (anime_cleaned['rating'] <= 10)]\n",
    "print(f\"7. Invalid ratings removed: {invalid_low + invalid_high:,}\")\n",
    "\n",
    "# 8. Handle missing members\n",
    "missing_members = anime_cleaned['members'].isnull().sum()\n",
    "anime_cleaned['members'] = anime_cleaned['members'].fillna(0)\n",
    "anime_cleaned['members'] = anime_cleaned['members'].astype(int)\n",
    "print(f\"8. Missing members filled with 0: {missing_members:,}\")\n",
    "\n",
    "# 9. Remove anime with very few members\n",
    "min_members_threshold = 10\n",
    "low_members = (anime_cleaned['members'] < min_members_threshold).sum()\n",
    "anime_cleaned = anime_cleaned[anime_cleaned['members'] >= min_members_threshold]\n",
    "print(f\"9. Anime with <{min_members_threshold} members removed: {low_members:,}\")\n",
    "\n",
    "# 10. Clean whitespace from text columns\n",
    "anime_cleaned['name'] = anime_cleaned['name'].str.strip()\n",
    "anime_cleaned['genre'] = anime_cleaned['genre'].str.strip()\n",
    "anime_cleaned['type'] = anime_cleaned['type'].str.strip()\n",
    "print(f\"10. Whitespace cleaned from text columns ✓\")\n",
    "\n",
    "# 11. Reset index\n",
    "anime_cleaned = anime_cleaned.reset_index(drop=True)\n",
    "print(f\"11. Index reset ✓\")\n",
    "\n",
    "# Final summary\n",
    "rows_removed = len(anime_raw) - len(anime_cleaned)\n",
    "print(f\"\\n--- ANIME DATASET CLEANING COMPLETE ---\")\n",
    "print(f\"Original: {len(anime_raw):,} rows\")\n",
    "print(f\"Cleaned: {len(anime_cleaned):,} rows\")\n",
    "print(f\"Removed: {rows_removed:,} rows ({(rows_removed/len(anime_raw)*100):.2f}%)\")\n",
    "print(f\"\\nMissing values: {anime_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"Duplicates: {anime_cleaned.duplicated().sum()}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n--- Sample of Cleaned Data ---\")\n",
    "print(anime_cleaned.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad58d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Clean Rating Dataset (Complete)\n",
    "\n",
    "**Objective:** Perform all rating dataset cleaning operations in one comprehensive step.\n",
    "\n",
    "**Cleaning Operations:**\n",
    "1. Remove duplicate ratings\n",
    "2. Remove rows with any missing values\n",
    "3. Remove -1 ratings (watched but not rated - unusable for training)\n",
    "4. Remove invalid ratings (outside 1-10 range)\n",
    "5. Remove ratings for anime that don't exist in cleaned anime dataset\n",
    "6. Remove users with very few ratings (<5 ratings)\n",
    "7. Ensure correct data types for all columns\n",
    "8. Reset index for clean structure\n",
    "\n",
    "**Rationale:** All cleaning operations combined for efficiency and clarity. Ensures data consistency between anime and rating datasets.\n",
    "\n",
    "**Expected Result:** Fully cleaned rating dataset aligned with anime dataset and ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c70169d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLEANING RATING DATASET (COMPLETE)\n",
      "============================================================\n",
      "\n",
      "Starting with: 7,813,737 rows\n",
      "\n",
      "1. Duplicates removed: 1\n",
      "   Remaining rows: 7,813,736\n",
      "\n",
      "2. Rows with missing values removed: 0\n",
      "   Remaining rows: 7,813,736\n",
      "\n",
      "3. Unrated entries (-1) removed: 1,476,496\n",
      "   Remaining rows: 6,337,240\n",
      "\n",
      "4. Invalid ratings removed: 0\n",
      "   Remaining rows: 6,337,240\n",
      "\n",
      "5. Ratings for non-existent anime removed: 7\n",
      "   Remaining rows: 6,337,233\n",
      "\n",
      "6. Ratings from users with <5 ratings removed: 18,810\n",
      "   Remaining rows: 6,318,423\n",
      "\n",
      "7. Data types corrected ✓\n",
      "8. Index reset ✓\n",
      "\n",
      "--- RATING DATASET CLEANING COMPLETE ---\n",
      "Original: 7,813,737 rows\n",
      "Cleaned: 6,318,423 rows\n",
      "Removed: 1,495,314 rows (19.14%)\n",
      "\n",
      "Missing values: 0\n",
      "Duplicates: 0\n",
      "Unique users: 60,970\n",
      "Unique anime: 9,924\n",
      "\n",
      "--- Sample of Cleaned Data ---\n",
      "   user_id  anime_id  rating\n",
      "0        3        20       8\n",
      "1        3       154       6\n",
      "2        3       170       9\n"
     ]
    }
   ],
   "source": [
    "# Complete rating dataset cleaning\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLEANING RATING DATASET (COMPLETE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a copy to preserve original\n",
    "rating_cleaned = rating_raw.copy()\n",
    "print(f\"\\nStarting with: {len(rating_cleaned):,} rows\")\n",
    "\n",
    "# 1. Remove duplicate ratings\n",
    "duplicates_count = rating_cleaned.duplicated().sum()\n",
    "rating_cleaned = rating_cleaned.drop_duplicates()\n",
    "print(f\"\\n1. Duplicates removed: {duplicates_count:,}\")\n",
    "print(f\"   Remaining rows: {len(rating_cleaned):,}\")\n",
    "\n",
    "# 2. Remove rows with missing values\n",
    "missing_before = rating_cleaned.isnull().sum().sum()\n",
    "rating_cleaned = rating_cleaned.dropna()\n",
    "print(f\"\\n2. Rows with missing values removed: {missing_before:,}\")\n",
    "print(f\"   Remaining rows: {len(rating_cleaned):,}\")\n",
    "\n",
    "# 3. Remove -1 ratings (watched but didn't rate)\n",
    "unrated = (rating_cleaned['rating'] == -1).sum()\n",
    "rating_cleaned = rating_cleaned[rating_cleaned['rating'] != -1]\n",
    "print(f\"\\n3. Unrated entries (-1) removed: {unrated:,}\")\n",
    "print(f\"   Remaining rows: {len(rating_cleaned):,}\")\n",
    "\n",
    "# 4. Remove invalid ratings (outside 1-10 range)\n",
    "invalid_low = (rating_cleaned['rating'] < 1).sum()\n",
    "invalid_high = (rating_cleaned['rating'] > 10).sum()\n",
    "rating_cleaned = rating_cleaned[(rating_cleaned['rating'] >= 1) & (rating_cleaned['rating'] <= 10)]\n",
    "print(f\"\\n4. Invalid ratings removed: {invalid_low + invalid_high:,}\")\n",
    "print(f\"   Remaining rows: {len(rating_cleaned):,}\")\n",
    "\n",
    "# 5. Remove ratings for anime not in cleaned anime dataset\n",
    "valid_anime_ids = set(anime_cleaned['anime_id'])\n",
    "before_filter = len(rating_cleaned)\n",
    "rating_cleaned = rating_cleaned[rating_cleaned['anime_id'].isin(valid_anime_ids)]\n",
    "removed = before_filter - len(rating_cleaned)\n",
    "print(f\"\\n5. Ratings for non-existent anime removed: {removed:,}\")\n",
    "print(f\"   Remaining rows: {len(rating_cleaned):,}\")\n",
    "\n",
    "# 6. Remove users with very few ratings (reduce noise)\n",
    "min_ratings_per_user = 5\n",
    "user_rating_counts = rating_cleaned['user_id'].value_counts()\n",
    "valid_users = user_rating_counts[user_rating_counts >= min_ratings_per_user].index\n",
    "before_user_filter = len(rating_cleaned)\n",
    "rating_cleaned = rating_cleaned[rating_cleaned['user_id'].isin(valid_users)]\n",
    "removed_users = before_user_filter - len(rating_cleaned)\n",
    "print(f\"\\n6. Ratings from users with <{min_ratings_per_user} ratings removed: {removed_users:,}\")\n",
    "print(f\"   Remaining rows: {len(rating_cleaned):,}\")\n",
    "\n",
    "# 7. Ensure correct data types\n",
    "rating_cleaned['user_id'] = rating_cleaned['user_id'].astype(int)\n",
    "rating_cleaned['anime_id'] = rating_cleaned['anime_id'].astype(int)\n",
    "rating_cleaned['rating'] = rating_cleaned['rating'].astype(int)\n",
    "print(f\"\\n7. Data types corrected ✓\")\n",
    "\n",
    "# 8. Reset index\n",
    "rating_cleaned = rating_cleaned.reset_index(drop=True)\n",
    "print(f\"8. Index reset ✓\")\n",
    "\n",
    "# Final summary\n",
    "rows_removed = len(rating_raw) - len(rating_cleaned)\n",
    "print(f\"\\n--- RATING DATASET CLEANING COMPLETE ---\")\n",
    "print(f\"Original: {len(rating_raw):,} rows\")\n",
    "print(f\"Cleaned: {len(rating_cleaned):,} rows\")\n",
    "print(f\"Removed: {rows_removed:,} rows ({(rows_removed/len(rating_raw)*100):.2f}%)\")\n",
    "print(f\"\\nMissing values: {rating_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"Duplicates: {rating_cleaned.duplicated().sum()}\")\n",
    "print(f\"Unique users: {rating_cleaned['user_id'].nunique():,}\")\n",
    "print(f\"Unique anime: {rating_cleaned['anime_id'].nunique():,}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n--- Sample of Cleaned Data ---\")\n",
    "print(rating_cleaned.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94567cc8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Data Quality Validation and Statistics\n",
    "\n",
    "**Objective:** Verify data quality and generate comprehensive statistics for both datasets.\n",
    "\n",
    "**Validation Checks:**\n",
    "1. Missing values count (should be 0)\n",
    "2. Duplicate count (should be 0)\n",
    "3. Data ranges and distributions\n",
    "4. Dataset relationships (user counts, anime counts)\n",
    "5. Data sparsity calculation\n",
    "\n",
    "**Purpose:** Ensure data is ready for algorithm implementation and understand dataset characteristics.\n",
    "\n",
    "**Expected Output:** Complete data quality report with statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75f436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY VALIDATION & STATISTICS\n",
      "============================================================\n",
      "\n",
      "--- ANIME DATASET QUALITY ---\n",
      "Total anime: 12,064\n",
      "Missing values: 0\n",
      "Duplicates: 0\n",
      "Unique anime IDs: 12,064\n",
      "Rating range: 1.67 - 10.00\n",
      "Average rating: 6.47\n",
      "Members range: 12 - 1,013,917\n",
      "\n",
      "Anime types distribution:\n",
      "type\n",
      "TV         3671\n",
      "OVA        3285\n",
      "Movie      2297\n",
      "Special    1671\n",
      "ONA         652\n",
      "Music       488\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- RATING DATASET QUALITY ---\n",
      "Total ratings: 6,318,423\n",
      "Missing values: 0\n",
      "Duplicates: 0\n",
      "Unique users: 60,970\n",
      "Unique anime rated: 9,924\n",
      "Rating range: 1 - 10\n",
      "Average user rating: 7.81\n",
      "Median user rating: 8.0\n",
      "\n",
      "--- DATASET RELATIONSHIPS ---\n",
      "Average ratings per user: 103.63\n",
      "Average ratings per anime: 636.68\n",
      "\n",
      "Data sparsity: 98.9557%\n",
      "(Higher = more sparse; typical for recommendation systems)\n",
      "\n",
      "--- RATING DISTRIBUTION ---\n",
      "rating\n",
      "1       16575\n",
      "2       23104\n",
      "3       41407\n",
      "4      104210\n",
      "5      282553\n",
      "6      637174\n",
      "7     1373584\n",
      "8     1642673\n",
      "9     1249433\n",
      "10     947710\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "✓ DATA QUALITY VALIDATION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive data quality validation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA QUALITY VALIDATION & STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- ANIME DATASET QUALITY ---\n",
    "print(\"\\n--- ANIME DATASET QUALITY ---\")\n",
    "print(f\"Total anime: {len(anime_cleaned):,}\")\n",
    "print(f\"Missing values: {anime_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"Duplicates: {anime_cleaned.duplicated().sum()}\")\n",
    "print(f\"Unique anime IDs: {anime_cleaned['anime_id'].nunique():,}\")\n",
    "print(f\"Rating range: {anime_cleaned['rating'].min():.2f} - {anime_cleaned['rating'].max():.2f}\")\n",
    "print(f\"Average rating: {anime_cleaned['rating'].mean():.2f}\")\n",
    "print(f\"Members range: {anime_cleaned['members'].min():,} - {anime_cleaned['members'].max():,}\")\n",
    "\n",
    "print(\"\\nAnime types distribution:\")\n",
    "print(anime_cleaned['type'].value_counts())\n",
    "\n",
    "# --- RATING DATASET QUALITY ---\n",
    "print(\"\\n--- RATING DATASET QUALITY ---\")\n",
    "print(f\"Total ratings: {len(rating_cleaned):,}\")\n",
    "print(f\"Missing values: {rating_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"Duplicates: {rating_cleaned.duplicated().sum()}\")\n",
    "print(f\"Unique users: {rating_cleaned['user_id'].nunique():,}\")\n",
    "print(f\"Unique anime rated: {rating_cleaned['anime_id'].nunique():,}\")\n",
    "print(f\"Rating range: {rating_cleaned['rating'].min()} - {rating_cleaned['rating'].max()}\")\n",
    "print(f\"Average user rating: {rating_cleaned['rating'].mean():.2f}\")\n",
    "print(f\"Median user rating: {rating_cleaned['rating'].median():.1f}\")\n",
    "\n",
    "# --- RELATIONSHIP STATISTICS ---\n",
    "print(\"\\n--- DATASET RELATIONSHIPS ---\")\n",
    "ratings_per_user = len(rating_cleaned) / rating_cleaned['user_id'].nunique()\n",
    "ratings_per_anime = len(rating_cleaned) / rating_cleaned['anime_id'].nunique()\n",
    "print(f\"Average ratings per user: {ratings_per_user:.2f}\")\n",
    "print(f\"Average ratings per anime: {ratings_per_anime:.2f}\")\n",
    "\n",
    "# Data sparsity calculation (important for recommendation systems)\n",
    "total_possible_ratings = rating_cleaned['user_id'].nunique() * rating_cleaned['anime_id'].nunique()\n",
    "sparsity = 1 - (len(rating_cleaned) / total_possible_ratings)\n",
    "print(f\"\\nData sparsity: {sparsity * 100:.4f}%\")\n",
    "print(f\"(Higher = more sparse; typical for recommendation systems)\")\n",
    "\n",
    "# Rating distribution\n",
    "print(\"\\n--- RATING DISTRIBUTION ---\")\n",
    "print(rating_cleaned['rating'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ DATA QUALITY VALIDATION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189536dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Save Cleaned CSV Files\n",
    "\n",
    "**Objective:** Save the cleaned datasets to CSV files.\n",
    "\n",
    "**Output Files:**\n",
    "- ../data/processed/anime_cleaned.csv - Cleaned anime dataset\n",
    "- ../data/processed/rating_cleaned.csv - Cleaned rating dataset\n",
    "- ../data/processed/combined_data.csv - Merged anime + rating dataset\n",
    "\n",
    "**Purpose:** Preserve cleaned data for future use and create combined dataset for algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e987a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING CLEANED CSV FILES\n",
      "============================================================\n",
      "\n",
      "✓ Ensured 'processed' folder exists\n",
      "✓ Anime dataset saved: ../data/processed/anime_cleaned.csv\n",
      "  Size: 12,064 rows × 7 columns\n",
      "✓ Rating dataset saved: ../data/processed/rating_cleaned.csv\n",
      "  Size: 6,318,423 rows × 3 columns\n",
      "\n",
      "--- Creating Combined Dataset ---\n",
      "Combined dataset shape: (6318423, 9)\n",
      "✓ Combined dataset saved: ../data/processed/combined_data.csv\n",
      "  Size: 6,318,423 rows × 9 columns\n",
      "\n",
      "--- FILE INFORMATION ---\n",
      "Anime CSV size: 0.87 MB\n",
      "Rating CSV size: 84.93 MB\n",
      "Combined CSV size: 588.85 MB\n",
      "Total size: 674.65 MB\n",
      "\n",
      "✓ All CSV files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned CSV files\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING CLEANED CSV FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "Path('../data/processed').mkdir(parents=True, exist_ok=True)\n",
    "print(\"\\n✓ Ensured 'processed' folder exists\")\n",
    "\n",
    "# Save cleaned anime dataset\n",
    "anime_cleaned.to_csv('../data/processed/anime_cleaned.csv', index=False, encoding='utf-8')\n",
    "print(f\"✓ Anime dataset saved: ../data/processed/anime_cleaned.csv\")\n",
    "print(f\"  Size: {len(anime_cleaned):,} rows × {anime_cleaned.shape[1]} columns\")\n",
    "\n",
    "# Save cleaned rating dataset\n",
    "rating_cleaned.to_csv('../data/processed/rating_cleaned.csv', index=False)\n",
    "print(f\"✓ Rating dataset saved: ../data/processed/rating_cleaned.csv\")\n",
    "print(f\"  Size: {len(rating_cleaned):,} rows × {rating_cleaned.shape[1]} columns\")\n",
    "\n",
    "# Create and save combined dataset\n",
    "print(f\"\\n--- Creating Combined Dataset ---\")\n",
    "combined_data = rating_cleaned.merge(\n",
    "    anime_cleaned, \n",
    "    on='anime_id', \n",
    "    how='inner'\n",
    ")\n",
    "print(f\"Combined dataset shape: {combined_data.shape}\")\n",
    "\n",
    "combined_data.to_csv('../data/processed/combined_data.csv', index=False, encoding='utf-8')\n",
    "print(f\"✓ Combined dataset saved: ../data/processed/combined_data.csv\")\n",
    "print(f\"  Size: {len(combined_data):,} rows × {combined_data.shape[1]} columns\")\n",
    "\n",
    "# Calculate file sizes\n",
    "import os\n",
    "anime_file_size = os.path.getsize('../data/processed/anime_cleaned.csv') / (1024 * 1024)\n",
    "rating_file_size = os.path.getsize('../data/processed/rating_cleaned.csv') / (1024 * 1024)\n",
    "combined_file_size = os.path.getsize('../data/processed/combined_data.csv') / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n--- FILE INFORMATION ---\")\n",
    "print(f\"Anime CSV size: {anime_file_size:.2f} MB\")\n",
    "print(f\"Rating CSV size: {rating_file_size:.2f} MB\")\n",
    "print(f\"Combined CSV size: {combined_file_size:.2f} MB\")\n",
    "print(f\"Total size: {anime_file_size + rating_file_size + combined_file_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n✓ All CSV files saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc916dfb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Generate ARFF Files for WEKA (Classification & Association)\n",
    "\n",
    "**Objective:** Convert cleaned datasets to ARFF format optimized for WEKA classification and association algorithms.\n",
    "\n",
    "**Output Files:**\n",
    "\n",
    "**For Classification (with class attribute):**\n",
    "1. anime_classification.arff - Anime data with rating as class\n",
    "2. rating_classification.arff - Rating data with rating as class\n",
    "3. combined_classification.arff - Combined data with rating as class ⭐\n",
    "\n",
    "**For Association Rules (discretized attributes):**\n",
    "4. anime_association.arff - Anime data in nominal format\n",
    "5. rating_association.arff - Rating data in nominal format\n",
    "6. combined_association.arff - Combined data in nominal format ⭐\n",
    "\n",
    "**Key Features:**\n",
    "- Classification files: numeric/nominal attributes with 'rating' as class\n",
    "- Association files: all attributes converted to nominal (required for Apriori/FPGrowth)\n",
    "- Proper discretization for continuous values\n",
    "- WEKA-compatible format\n",
    "\n",
    "**Purpose:** Enable classification (J48, Bayes, OneR) and association (Apriori, FPGrowth) in WEKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80bd61f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING ARFF FILES FOR WEKA\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PART 1: CREATING CLASSIFICATION ARFF FILES\n",
      "============================================================\n",
      "\n",
      "--- Anime Classification ARFF ---\n",
      "✓ Created: ../data/processed/anime_classification.arff\n",
      "\n",
      "--- Rating Classification ARFF ---\n",
      "⚠ Rating dataset is large (6,318,423 rows)\n",
      "  Sampling 100,000 rows for WEKA performance\n",
      "✓ Created: ../data/processed/rating_classification.arff\n",
      "\n",
      "--- Combined Classification ARFF ---\n",
      "⚠ Combined dataset is large (6,318,423 rows)\n",
      "  Sampling 100,000 rows for WEKA performance\n",
      "✓ Created: ../data/processed/combined_classification.arff\n",
      "\n",
      "============================================================\n",
      "PART 2: CREATING ASSOCIATION ARFF FILES\n",
      "============================================================\n",
      "\n",
      "--- Anime Association ARFF ---\n",
      "✓ Created: ../data/processed/anime_association.arff\n",
      "\n",
      "--- Rating Association ARFF ---\n",
      "✓ Created: ../data/processed/rating_association.arff\n",
      "\n",
      "--- Combined Association ARFF ---\n",
      "✓ Created: ../data/processed/combined_association.arff\n",
      "\n",
      "============================================================\n",
      "ARFF FILE GENERATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "--- CLASSIFICATION ARFF FILES (For J48, Bayes, OneR, etc.) ---\n",
      "1. ../data/processed/anime_classification.arff\n",
      "   Rows: 12,064 | Class: rating\n",
      "\n",
      "2. ../data/processed/rating_classification.arff\n",
      "   Rows: 100,000 | Class: rating\n",
      "\n",
      "3. ../data/processed/combined_classification.arff ⭐ (RECOMMENDED)\n",
      "   Rows: 100,000 | Class: user_rating\n",
      "\n",
      "--- ASSOCIATION ARFF FILES (For Apriori, FPGrowth) ---\n",
      "4. ../data/processed/anime_association.arff\n",
      "   Rows: 12,064 | All attributes: nominal\n",
      "\n",
      "5. ../data/processed/rating_association.arff\n",
      "   Rows: 100,000 | All attributes: nominal\n",
      "\n",
      "6. ../data/processed/combined_association.arff ⭐ (RECOMMENDED)\n",
      "   Rows: 100,000 | All attributes: nominal\n",
      "\n",
      "--- DISCRETIZATION RULES (for Association files) ---\n",
      "• Rating: Low(1-4), Medium(5-7), High(8-10)\n",
      "• Episodes: Short(1-12), Medium(13-26), Long(27+)\n",
      "• Members: Low(<1000), Medium(1000-10000), High(>10000)\n",
      "\n",
      "--- HOW TO USE IN WEKA ---\n",
      "\n",
      "FOR CLASSIFICATION:\n",
      "  1. Open WEKA Explorer\n",
      "  2. Load: combined_classification.arff\n",
      "  3. Go to 'Classify' tab\n",
      "  4. Choose classifier: J48, NaiveBayes, OneR, etc.\n",
      "  5. Test options: Cross-validation (10 folds)\n",
      "  6. Click 'Start' to run\n",
      "\n",
      "FOR ASSOCIATION RULES:\n",
      "  1. Open WEKA Explorer\n",
      "  2. Load: combined_association.arff\n",
      "  3. Go to 'Associate' tab\n",
      "  4. Choose: Apriori or FPGrowth\n",
      "  5. Set parameters (minSupport, minConfidence)\n",
      "  6. Click 'Start' to mine rules\n",
      "\n",
      "✓ All ARFF files ready for WEKA!\n",
      "✓ 6 files generated (3 classification + 3 association)\n"
     ]
    }
   ],
   "source": [
    "# Generate ARFF files for WEKA (Classification & Association)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING ARFF FILES FOR WEKA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_classification_arff(dataframe, filename, relation_name, class_attribute='rating'):\n",
    "    \"\"\"\n",
    "    Create ARFF file for WEKA classification algorithms\n",
    "    The class_attribute will be placed last and marked as class\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        # Write relation name\n",
    "        f.write(f\"@RELATION {relation_name}\\n\\n\")\n",
    "        \n",
    "        # Reorder columns to put class attribute last\n",
    "        cols = [col for col in dataframe.columns if col != class_attribute]\n",
    "        if class_attribute in dataframe.columns:\n",
    "            cols.append(class_attribute)\n",
    "        df_reordered = dataframe[cols]\n",
    "        \n",
    "        # Write attributes\n",
    "        for col in df_reordered.columns:\n",
    "            dtype = df_reordered[col].dtype\n",
    "            \n",
    "            if col == class_attribute:\n",
    "                # Class attribute - use nominal for rating (1-10)\n",
    "                unique_vals = sorted(df_reordered[col].unique())\n",
    "                vals = ','.join([str(int(v)) for v in unique_vals if pd.notna(v)])\n",
    "                f.write(f\"@ATTRIBUTE {col} {{{vals}}}\\n\")\n",
    "            elif dtype == 'object':\n",
    "                unique_vals = df_reordered[col].unique()\n",
    "                if len(unique_vals) > 100:\n",
    "                    f.write(f\"@ATTRIBUTE {col} string\\n\")\n",
    "                else:\n",
    "                    vals = ','.join([f\"'{str(v)}'\" for v in unique_vals if pd.notna(v)])\n",
    "                    f.write(f\"@ATTRIBUTE {col} {{{vals}}}\\n\")\n",
    "            elif dtype in ['int64', 'int32', 'float64', 'float32']:\n",
    "                f.write(f\"@ATTRIBUTE {col} numeric\\n\")\n",
    "            else:\n",
    "                f.write(f\"@ATTRIBUTE {col} string\\n\")\n",
    "        \n",
    "        # Write data\n",
    "        f.write(\"\\n@DATA\\n\")\n",
    "        for idx, row in df_reordered.iterrows():\n",
    "            row_data = []\n",
    "            for col in df_reordered.columns:\n",
    "                val = row[col]\n",
    "                if pd.isna(val):\n",
    "                    row_data.append('?')\n",
    "                elif df_reordered[col].dtype == 'object':\n",
    "                    val_str = str(val).replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
    "                    if ',' in val_str or ' ' in val_str:\n",
    "                        row_data.append(f\"'{val_str}'\")\n",
    "                    else:\n",
    "                        row_data.append(f\"'{val_str}'\")\n",
    "                else:\n",
    "                    row_data.append(str(val))\n",
    "            f.write(','.join(row_data) + '\\n')\n",
    "    \n",
    "    print(f\"✓ Created: {filename}\")\n",
    "\n",
    "def create_association_arff(dataframe, filename, relation_name):\n",
    "    \"\"\"\n",
    "    Create ARFF file for WEKA association algorithms (Apriori, FPGrowth)\n",
    "    All attributes must be nominal (categorical)\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        # Write relation name\n",
    "        f.write(f\"@RELATION {relation_name}\\n\\n\")\n",
    "        \n",
    "        # Discretize numeric columns\n",
    "        df_nominal = dataframe.copy()\n",
    "        \n",
    "        for col in df_nominal.columns:\n",
    "            dtype = df_nominal[col].dtype\n",
    "            \n",
    "            if dtype in ['int64', 'int32', 'float64', 'float32']:\n",
    "                # Discretize numeric attributes\n",
    "                if col == 'rating':\n",
    "                    # Rating 1-10: group into Low(1-4), Medium(5-7), High(8-10)\n",
    "                    df_nominal[col] = pd.cut(df_nominal[col], \n",
    "                                            bins=[0, 4, 7, 10], \n",
    "                                            labels=['Low', 'Medium', 'High'],\n",
    "                                            include_lowest=True)\n",
    "                elif col == 'episodes':\n",
    "                    # Episodes: Short(1-12), Medium(13-26), Long(27+)\n",
    "                    df_nominal[col] = pd.cut(df_nominal[col], \n",
    "                                            bins=[-1, 12, 26, float('inf')], \n",
    "                                            labels=['Short', 'Medium', 'Long'])\n",
    "                elif col == 'members':\n",
    "                    # Members: Low(<1000), Medium(1000-10000), High(>10000)\n",
    "                    df_nominal[col] = pd.cut(df_nominal[col], \n",
    "                                            bins=[-1, 1000, 10000, float('inf')], \n",
    "                                            labels=['Low', 'Medium', 'High'])\n",
    "                elif col == 'anime_id' or col == 'user_id':\n",
    "                    # Keep IDs as strings\n",
    "                    df_nominal[col] = df_nominal[col].astype(str)\n",
    "                else:\n",
    "                    # Generic discretization for other numeric columns\n",
    "                    df_nominal[col] = pd.cut(df_nominal[col], \n",
    "                                            bins=5, \n",
    "                                            labels=['VeryLow', 'Low', 'Medium', 'High', 'VeryHigh'])\n",
    "        \n",
    "        # Write attributes (all nominal)\n",
    "        for col in df_nominal.columns:\n",
    "            unique_vals = df_nominal[col].unique()\n",
    "            # Remove NaN from unique values\n",
    "            unique_vals = [v for v in unique_vals if pd.notna(v)]\n",
    "            \n",
    "            if len(unique_vals) > 0:\n",
    "                if df_nominal[col].dtype == 'object' or df_nominal[col].dtype.name == 'category':\n",
    "                    vals = ','.join([f\"'{str(v)}'\" for v in unique_vals])\n",
    "                    f.write(f\"@ATTRIBUTE {col} {{{vals}}}\\n\")\n",
    "                else:\n",
    "                    vals = ','.join([str(v) for v in unique_vals])\n",
    "                    f.write(f\"@ATTRIBUTE {col} {{{vals}}}\\n\")\n",
    "        \n",
    "        # Write data\n",
    "        f.write(\"\\n@DATA\\n\")\n",
    "        for idx, row in df_nominal.iterrows():\n",
    "            row_data = []\n",
    "            for col in df_nominal.columns:\n",
    "                val = row[col]\n",
    "                if pd.isna(val):\n",
    "                    row_data.append('?')\n",
    "                else:\n",
    "                    val_str = str(val).replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
    "                    if ',' in val_str or ' ' in val_str:\n",
    "                        row_data.append(f\"'{val_str}'\")\n",
    "                    else:\n",
    "                        row_data.append(f\"'{val_str}'\")\n",
    "            f.write(','.join(row_data) + '\\n')\n",
    "    \n",
    "    print(f\"✓ Created: {filename}\")\n",
    "\n",
    "# ====================================================\n",
    "# PART 1: CLASSIFICATION ARFF FILES\n",
    "# ====================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PART 1: CREATING CLASSIFICATION ARFF FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Anime Classification ARFF\n",
    "print(\"\\n--- Anime Classification ARFF ---\")\n",
    "anime_class_path = '../data/processed/anime_classification.arff'\n",
    "create_classification_arff(anime_cleaned, anime_class_path, 'anime_classification', 'rating')\n",
    "\n",
    "# 2. Rating Classification ARFF (sample if too large)\n",
    "print(\"\\n--- Rating Classification ARFF ---\")\n",
    "rating_sample = rating_cleaned\n",
    "if len(rating_cleaned) > 100000:\n",
    "    print(f\"⚠ Rating dataset is large ({len(rating_cleaned):,} rows)\")\n",
    "    print(f\"  Sampling 100,000 rows for WEKA performance\")\n",
    "    rating_sample = rating_cleaned.sample(n=100000, random_state=42)\n",
    "\n",
    "rating_class_path = '../data/processed/rating_classification.arff'\n",
    "create_classification_arff(rating_sample, rating_class_path, 'rating_classification', 'rating')\n",
    "\n",
    "# 3. Combined Classification ARFF (MOST IMPORTANT)\n",
    "print(\"\\n--- Combined Classification ARFF ---\")\n",
    "combined_sample = combined_data\n",
    "if len(combined_data) > 100000:\n",
    "    print(f\"⚠ Combined dataset is large ({len(combined_data):,} rows)\")\n",
    "    print(f\"  Sampling 100,000 rows for WEKA performance\")\n",
    "    combined_sample = combined_data.sample(n=100000, random_state=42)\n",
    "\n",
    "# For combined data, use user rating as class (rename to user_rating for clarity)\n",
    "combined_for_class = combined_sample.copy()\n",
    "if 'rating_x' in combined_for_class.columns:\n",
    "    combined_for_class = combined_for_class.rename(columns={'rating_x': 'user_rating', 'rating_y': 'anime_rating'})\n",
    "    class_attr = 'user_rating'\n",
    "else:\n",
    "    class_attr = 'rating'\n",
    "\n",
    "combined_class_path = '../data/processed/combined_classification.arff'\n",
    "create_classification_arff(combined_for_class, combined_class_path, 'combined_classification', class_attr)\n",
    "\n",
    "# ====================================================\n",
    "# PART 2: ASSOCIATION ARFF FILES\n",
    "# ====================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PART 2: CREATING ASSOCIATION ARFF FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 4. Anime Association ARFF\n",
    "print(\"\\n--- Anime Association ARFF ---\")\n",
    "anime_assoc_path = '../data/processed/anime_association.arff'\n",
    "create_association_arff(anime_cleaned, anime_assoc_path, 'anime_association')\n",
    "\n",
    "# 5. Rating Association ARFF\n",
    "print(\"\\n--- Rating Association ARFF ---\")\n",
    "rating_assoc_path = '../data/processed/rating_association.arff'\n",
    "create_association_arff(rating_sample, rating_assoc_path, 'rating_association')\n",
    "\n",
    "# 6. Combined Association ARFF (MOST IMPORTANT)\n",
    "print(\"\\n--- Combined Association ARFF ---\")\n",
    "combined_assoc_path = '../data/processed/combined_association.arff'\n",
    "create_association_arff(combined_sample, combined_assoc_path, 'combined_association')\n",
    "\n",
    "# ====================================================\n",
    "# SUMMARY\n",
    "# ====================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ARFF FILE GENERATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- CLASSIFICATION ARFF FILES (For J48, Bayes, OneR, etc.) ---\")\n",
    "print(f\"1. {anime_class_path}\")\n",
    "print(f\"   Rows: {len(anime_cleaned):,} | Class: rating\")\n",
    "print(f\"\\n2. {rating_class_path}\")\n",
    "print(f\"   Rows: {len(rating_sample):,} | Class: rating\")\n",
    "print(f\"\\n3. {combined_class_path} ⭐ (RECOMMENDED)\")\n",
    "print(f\"   Rows: {len(combined_sample):,} | Class: {class_attr}\")\n",
    "\n",
    "print(\"\\n--- ASSOCIATION ARFF FILES (For Apriori, FPGrowth) ---\")\n",
    "print(f\"4. {anime_assoc_path}\")\n",
    "print(f\"   Rows: {len(anime_cleaned):,} | All attributes: nominal\")\n",
    "print(f\"\\n5. {rating_assoc_path}\")\n",
    "print(f\"   Rows: {len(rating_sample):,} | All attributes: nominal\")\n",
    "print(f\"\\n6. {combined_assoc_path} ⭐ (RECOMMENDED)\")\n",
    "print(f\"   Rows: {len(combined_sample):,} | All attributes: nominal\")\n",
    "\n",
    "print(\"\\n--- DISCRETIZATION RULES (for Association files) ---\")\n",
    "print(\"• Rating: Low(1-4), Medium(5-7), High(8-10)\")\n",
    "print(\"• Episodes: Short(1-12), Medium(13-26), Long(27+)\")\n",
    "print(\"• Members: Low(<1000), Medium(1000-10000), High(>10000)\")\n",
    "\n",
    "print(\"\\n--- HOW TO USE IN WEKA ---\")\n",
    "print(\"\\nFOR CLASSIFICATION:\")\n",
    "print(\"  1. Open WEKA Explorer\")\n",
    "print(\"  2. Load: combined_classification.arff\")\n",
    "print(\"  3. Go to 'Classify' tab\")\n",
    "print(\"  4. Choose classifier: J48, NaiveBayes, OneR, etc.\")\n",
    "print(\"  5. Test options: Cross-validation (10 folds)\")\n",
    "print(\"  6. Click 'Start' to run\")\n",
    "\n",
    "print(\"\\nFOR ASSOCIATION RULES:\")\n",
    "print(\"  1. Open WEKA Explorer\")\n",
    "print(\"  2. Load: combined_association.arff\")\n",
    "print(\"  3. Go to 'Associate' tab\")\n",
    "print(\"  4. Choose: Apriori or FPGrowth\")\n",
    "print(\"  5. Set parameters (minSupport, minConfidence)\")\n",
    "print(\"  6. Click 'Start' to mine rules\")\n",
    "\n",
    "print(\"\\n✓ All ARFF files ready for WEKA!\")\n",
    "print(\"✓ 6 files generated (3 classification + 3 association)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

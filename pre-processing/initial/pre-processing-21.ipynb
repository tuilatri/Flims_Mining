{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Pre-processing 21: Baseline / Basic Preprocessing\n",
                "\n",
                "This notebook implements a simplified preprocessing pipeline (Version 21) to serve as a baseline for comparison. Unlike Version 20, this version avoids advanced techniques like clustering and complex encoding, opting instead for basic binning and simplification to intentionally achieve lower accuracy in supervised learning tasks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "\n",
                "# Paths\n",
                "RAW_DIR = \"../dataset/raw\"\n",
                "OUT_DIR = \".\"\n",
                "IMG_DIR = \"images\"\n",
                "\n",
                "if not os.path.exists(IMG_DIR):\n",
                "    os.makedirs(IMG_DIR)\n",
                "\n",
                "print(\"Loading data...\")\n",
                "anime = pd.read_csv(os.path.join(RAW_DIR, \"anime.csv\"))\n",
                "rating = pd.read_csv(os.path.join(RAW_DIR, \"rating.csv\"))\n",
                "\n",
                "# ==========================================\n",
                "# Anime Cleaning (Basic/Worse Version)\n",
                "# ==========================================\n",
                "print(\"Cleaning Anime...\")\n",
                "\n",
                "# 1. Genre: Take only the first genre (Loss of information)\n",
                "# Fill NA first\n",
                "anime['genre'] = anime['genre'].fillna('Unknown')\n",
                "# Split and take first\n",
                "anime['genre_encoded'] = anime['genre'].apply(lambda x: x.split(',')[0].strip())\n",
                "\n",
                "# 2. Type: Fill NA\n",
                "anime['type'] = anime['type'].fillna('Unknown')\n",
                "\n",
                "# 3. Episodes: Handle Unknown -> Median, then Binning\n",
                "# Replace Unknown with NaN\n",
                "anime['episodes'] = anime['episodes'].replace('Unknown', np.nan)\n",
                "# Convert to numeric\n",
                "anime['episodes'] = anime['episodes'].astype(float)\n",
                "# Fill NaN with median\n",
                "anime['episodes'] = anime['episodes'].fillna(anime['episodes'].median())\n",
                "\n",
                "# Binning Logic for Episodes (Target Class in User's Example)\n",
                "def bin_episodes(row):\n",
                "    # If it's a movie/special type, categorize as such\n",
                "    if row['type'] in ['Movie', 'Special', 'OVA', 'ONA', 'Music']:\n",
                "        return 'Movie/Special'\n",
                "    \n",
                "    x = row['episodes']\n",
                "    if x <= 13:\n",
                "        return 'Short_Series'\n",
                "    elif x <= 26:\n",
                "        return 'Medium_Series'\n",
                "    elif x <= 100:\n",
                "        return 'Long_Series'\n",
                "    else:\n",
                "        return 'Very_Long_Series'\n",
                "\n",
                "anime['episodes_encoded'] = anime.apply(bin_episodes, axis=1)\n",
                "\n",
                "# 4. Anime Rating: Simple Binning\n",
                "# Fill NaN\n",
                "anime['rating'] = anime['rating'].fillna(anime['rating'].median())\n",
                "\n",
                "def bin_anime_rating(x):\n",
                "    if x < 6.0:\n",
                "        return 'Low'\n",
                "    elif x < 8.0:\n",
                "        return 'Average'\n",
                "    else:\n",
                "        return 'High'\n",
                "\n",
                "anime['anime_rating_encoded'] = anime['rating'].apply(bin_anime_rating)\n",
                "\n",
                "# 5. Members: Simple Binning\n",
                "def bin_members(x):\n",
                "    if x < 10000:\n",
                "        return 'Low'\n",
                "    elif x < 100000:\n",
                "        return 'Medium'\n",
                "    else:\n",
                "        return 'High'\n",
                "\n",
                "anime['members_encoded'] = anime['members'].apply(bin_members)\n",
                "\n",
                "# Select columns for anime-cleaned.csv\n",
                "# Keep original IDs for merging, but output specific columns\n",
                "anime_out = anime[['anime_id', 'name', 'genre_encoded', 'type', 'episodes_encoded', 'anime_rating_encoded', 'members_encoded']]\n",
                "anime_out.to_csv(\"anime-cleaned.csv\", index=False)\n",
                "print(\"Saved anime-cleaned.csv\")\n",
                "\n",
                "# ==========================================\n",
                "# Rating Cleaning (Basic/Worse Version)\n",
                "# ==========================================\n",
                "print(\"Cleaning Rating...\")\n",
                "\n",
                "# 1. User Rating: Handle -1 and Bin\n",
                "# -1 in dataset often means \"watched but not rated\". We will treat it as a category \"No_Rating\".\n",
                "def bin_user_rating(x):\n",
                "    if x == -1:\n",
                "        return 'No_Rating'\n",
                "    elif x < 6:\n",
                "        return 'Low'\n",
                "    elif x < 8:\n",
                "        return 'Average'\n",
                "    else:\n",
                "        return 'High'\n",
                "\n",
                "rating['user_rating_encoded'] = rating['rating'].apply(bin_user_rating)\n",
                "\n",
                "# Select columns for rating-cleaned.csv\n",
                "rating_out = rating[['user_id', 'anime_id', 'user_rating_encoded']]\n",
                "rating_out.to_csv(\"rating-cleaned.csv\", index=False)\n",
                "print(\"Saved rating-cleaned.csv\")\n",
                "\n",
                "# ==========================================\n",
                "# Combined\n",
                "# ==========================================\n",
                "print(\"Merging...\")\n",
                "combined = pd.merge(rating_out, anime_out, on='anime_id', how='left')\n",
                "# Drop rows where anime info might be missing (if any)\n",
                "combined = combined.dropna()\n",
                "\n",
                "combined.to_csv(\"combined.csv\", index=False)\n",
                "print(\"Saved combined.csv\")\n",
                "\n",
                "# ==========================================\n",
                "# Visualization\n",
                "# ==========================================\n",
                "print(\"Generating Images...\")\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.countplot(data=anime_out, x='anime_rating_encoded')\n",
                "plt.title('Distribution of Anime Ratings (Encoded)')\n",
                "plt.savefig(os.path.join(IMG_DIR, \"anime_rating_dist.png\"))\n",
                "plt.close()\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.countplot(data=anime_out, x='episodes_encoded')\n",
                "plt.title('Distribution of Episode Lengths (Encoded)')\n",
                "plt.xticks(rotation=45)\n",
                "plt.savefig(os.path.join(IMG_DIR, \"episode_len_dist.png\"))\n",
                "plt.close()\n",
                "\n",
                "# ==========================================\n",
                "# ARFF Generation\n",
                "# ==========================================\n",
                "print(\"Generating ARFFs...\")\n",
                "\n",
                "def to_arff(df, filename, relation_name):\n",
                "    # Filter out columns that are not useful for Weka (IDs, Names) if they exist\n",
                "    # But for the specific requested files, we might need to be careful.\n",
                "    # We will drop IDs for the ARFF to ensure Weka can run algorithms without crashing on high cardinality.\n",
                "    \n",
                "    cols_to_write = [c for c in df.columns if c not in ['anime_id', 'user_id', 'name']]\n",
                "    \n",
                "    with open(filename, 'w', encoding='utf-8') as f:\n",
                "        f.write(f\"@RELATION {relation_name}\\n\\n\")\n",
                "        \n",
                "        for col in cols_to_write:\n",
                "            # Check dtype\n",
                "            if df[col].dtype == 'object' or df[col].dtype.name == 'category':\n",
                "                # Nominal\n",
                "                unique_vals = df[col].unique()\n",
                "                # Clean values for ARFF (remove quotes, handle spaces)\n",
                "                clean_vals = []\n",
                "                for v in unique_vals:\n",
                "                    s = str(v)\n",
                "                    # Escape single quotes\n",
                "                    s = s.replace(\"'\", \"\\\\'\")\n",
                "                    # Quote if contains space or special chars\n",
                "                    if ' ' in s or ',' in s or '{' in s or '}' in s or '?' in s:\n",
                "                        s = f\"'{s}'\"\n",
                "                    clean_vals.append(s)\n",
                "                \n",
                "                # Join\n",
                "                vals_str = \",\".join(clean_vals)\n",
                "                f.write(f\"@ATTRIBUTE {col} {{{vals_str}}}\\n\")\n",
                "            else:\n",
                "                # Numeric\n",
                "                f.write(f\"@ATTRIBUTE {col} NUMERIC\\n\")\n",
                "        \n",
                "        f.write(\"\\n@DATA\\n\")\n",
                "        \n",
                "        # Write data\n",
                "        # It's faster to convert to string and join\n",
                "        # But we need to handle the quoting logic again.\n",
                "        # Let's use a simpler approach for writing data\n",
                "        \n",
                "        for i, row in df[cols_to_write].iterrows():\n",
                "            line_vals = []\n",
                "            for col in cols_to_write:\n",
                "                val = row[col]\n",
                "                s = str(val)\n",
                "                s = s.replace(\"'\", \"\\\\'\")\n",
                "                if ' ' in s or ',' in s or '{' in s or '}' in s or '?' in s:\n",
                "                    s = f\"'{s}'\"\n",
                "                line_vals.append(s)\n",
                "            f.write(\",\".join(line_vals) + \"\\n\")\n",
                "\n",
                "# 1. anime-cleaned.arff\n",
                "to_arff(anime_out, \"anime-cleaned.arff\", \"anime_data\")\n",
                "\n",
                "# 2. rating-cleaned.arff\n",
                "# Sampling rating because it's huge (7M rows)\n",
                "rating_sampled = rating_out.sample(n=100000, random_state=42)\n",
                "to_arff(rating_sampled, \"rating-cleaned.arff\", \"rating_data\")\n",
                "\n",
                "# 3. combined-cleaned.arff\n",
                "# User requested sample around 100k\n",
                "combined_sampled = combined.sample(n=10000, random_state=42)\n",
                "to_arff(combined_sampled, \"combined-cleaned.arff\", \"combined_data\")\n",
                "\n",
                "print(\"Done.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
